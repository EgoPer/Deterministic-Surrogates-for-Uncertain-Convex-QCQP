{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009c6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wishart, random_correlation\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a14f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem generation parameters\n",
    "seed = 42\n",
    "n_vars = 5\n",
    "n_context = 4\n",
    "wishart_df = 50\n",
    "datset_size = 100\n",
    "test_proportion = 0.3\n",
    "\n",
    "# Optimisation problem parameters\n",
    "risk_limit = 0.4\n",
    "robustness_parameter = 1\n",
    "penalty_coefficient = 1\n",
    "\n",
    "# Prediction model hyperparameters\n",
    "hidden_size = 10\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd8a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalProcessGenerator:\n",
    "    \n",
    "    def __init__(self, n_vars, n_context, seed):\n",
    "        self.n_vars = n_vars\n",
    "        self.n_context = n_context\n",
    "        self.seed = seed\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        self.correlation = self.generate_correlation()\n",
    "        self.conditional_variance_weights = [np.random.rand(self.n_context) for _ in range(n_vars)]\n",
    "        self.conditional_variance_weights = [weights/np.sum(weights) for weights in self.conditional_variance_weights]\n",
    "        \n",
    "    def generate_correlation(self):\n",
    "        np.random.seed(self.seed)\n",
    "        generated_correlation_eig = np.random.rand(n_vars)\n",
    "        generated_correlation_eig = generated_correlation_eig/np.sum(generated_correlation_eig)\n",
    "        generated_correlation_eig *= n_vars\n",
    "        correlation = random_correlation.rvs(generated_correlation_eig)\n",
    "        return correlation\n",
    "    \n",
    "    def conditional_covariance(self, context):\n",
    "        conditional_variances = np.array([context @ weights for weights in self.conditional_variance_weights])\n",
    "        inverse_square_root_variances = np.sqrt(1/conditional_variances)\n",
    "        matrix_of_isqv = np.diag(inverse_square_root_variances) # matrix of inverse square root variances as diagonal\n",
    "        conditional_covariance = matrix_of_isqv@self.correlation@matrix_of_isqv\n",
    "        return conditional_covariance\n",
    "    \n",
    "    \n",
    "    def generate_context(self, size, lb = 0.1, ub = 1.):\n",
    "        # generates context z for problem\n",
    "        # z should be > 0 so the conditional variances make sense (convex combinations)\n",
    "        assert lb > 0, \"the lower bound of the generated context has to be greater than 0\"\n",
    "        \n",
    "        np.random.seed(self.seed)\n",
    "        z = np.random.uniform(low =lb, high = ub, size = (size,n_context))\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def generate_dataset(self, size, lb = 0.1, ub = 1., wishart_df = 100):\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        z = self.generate_context(size = size, lb = lb, ub = ub)\n",
    "        \n",
    "        conditional_covariances = np.apply_along_axis(dgp.conditional_covariance, 1, z)\n",
    "        \n",
    "        wishart_outcomes = [wishart(df = wishart_df, scale=conditional_covariances[i,...]).rvs()/wishart_df for \n",
    "                            i in range(size)\n",
    "                           ]\n",
    "        \n",
    "        wishart_outcomes = np.array(wishart_outcomes)\n",
    "        \n",
    "        # Fixed objective throughout\n",
    "        c = np.random.rand(n_vars,1)\n",
    "        \n",
    "        return z, wishart_outcomes, c\n",
    "    \n",
    "## Build prediction model that outputs symmetric matrices and in this case a vector for the cost function\n",
    "class NNMatrixOutput(nn.Module):\n",
    "    def __init__(self, n_context, hidden_size, n_vars):\n",
    "        super(NNMatrixOutput, self).__init__()\n",
    "        assert n_vars >= 2, \"not designed for outputing scalars\"\n",
    "        self.n_vars = n_vars\n",
    "        self.latent = nn.Linear(n_context, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.diagonal = nn.Linear(hidden_size, n_vars)\n",
    "        \n",
    "        n_lower_triangular_elements = int(n_vars*(n_vars - 1)/2)\n",
    "        self.lower_triangular = nn.Linear(hidden_size,n_lower_triangular_elements)\n",
    "        \n",
    "        self.c_predict = nn.Linear(hidden_size, n_vars)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \n",
    "        z = self.latent(z)\n",
    "        z = self.relu(z)\n",
    "        diagonal = self.diagonal(z)\n",
    "        lower_triangular = self.lower_triangular(z)\n",
    "        \n",
    "        # construct predicted matrix \n",
    "        matrix = torch.diag_embed(diagonal)\n",
    "        tind = torch.tril_indices(diagonal.shape[-1],diagonal.shape[-1],offset = -1)\n",
    "\n",
    "        matrix[...,tind[0],tind[1]] = lower_triangular\n",
    "        matrix[...,tind[1],tind[0]] = lower_triangular\n",
    "        \n",
    "        # predict conditional cost vector\n",
    "        chat = self.c_predict(z).unsqueeze(-1)\n",
    "\n",
    "        return {\"Sigma\" : matrix, \"c\" : chat}\n",
    "    \n",
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, z, Sigma):\n",
    "        self.z = torch.tensor(z).float()\n",
    "        self.Sigma = torch.tensor(Sigma).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.z.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'z': self.z[idx], 'Sigma': self.Sigma[idx]}\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858b8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build parameterised optimisation problem\n",
    "## Construct ROB_1 and ROB_2\n",
    "\n",
    "# outer problem, ROB_1\n",
    "x_det = cp.Variable((n_vars,1), name= \"decision\")\n",
    "# Define parameters for problem (the variables of interest for optimisation)\n",
    "c_param = [cp.Parameter((n_vars,1))]\n",
    "Sigma_sqrt_param = [cp.Parameter((n_vars,n_vars))] # Needs to be defined this way for cvxpy DPP compliance https://www.cvxpy.org/tutorial/advanced/index.html\n",
    "\n",
    "# construct problem\n",
    "constraints = []\n",
    "constraints += [cp.sum_squares(Sigma_sqrt_param[0]@x_det) <= risk_limit]\n",
    "constraints += [cp.sum(x_det) <= 1]\n",
    "constraints += [x_det >= 0]\n",
    "objective = cp.Minimize(-c_param[0].T@x_det)\n",
    "\n",
    "outer_problem = cp.Problem(objective,constraints)\n",
    "\n",
    "ROB_1 = CvxpyLayer(outer_problem, parameters=c_param + Sigma_sqrt_param, variables=[x_det])\n",
    "\n",
    "# inner problem, ROB_2\n",
    "x_fixed = cp.Parameter((n_vars,1))\n",
    "x_xt_fixed = cp.Parameter((n_vars,n_vars))\n",
    "\n",
    "sigma_realisation = cp.Parameter((n_vars,n_vars))\n",
    "sigma_addition = cp.Variable((n_vars,n_vars), symmetric = True)\n",
    "sigma_worst_case_in_neighbourhood =cp.Variable((n_vars,n_vars))\n",
    "constraints = []\n",
    "constraints += [sigma_worst_case_in_neighbourhood == sigma_realisation + sigma_addition]\n",
    "constraints += [cp.norm(sigma_addition,\"fro\") <= robustness_parameter]\n",
    "\n",
    "objective_function = penalty_coefficient*(cp.sum(cp.multiply(x_xt_fixed,sigma_worst_case_in_neighbourhood))-risk_limit)\n",
    "objective = cp.Maximize(objective_function)\n",
    "\n",
    "inner_problem = cp.Problem(objective,constraints)\n",
    "\n",
    "ROB_2 = CvxpyLayer(inner_problem, parameters=[x_xt_fixed,sigma_realisation], variables=[sigma_addition,sigma_worst_case_in_neighbourhood])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6abc75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build experiment pipeline\n",
    "dgp = ConditionalProcessGenerator(n_vars = n_vars, n_context = n_context, seed = seed)\n",
    "\n",
    "# Generate dataset\n",
    "z, Sigmas, c = dgp.generate_dataset(size = datset_size)\n",
    "c = torch.tensor(c).float()\n",
    "# Split into training and test sets and feed into dataloaders\n",
    "z_train, z_test, Sigmas_train, Sigmas_test = train_test_split(z, Sigmas, \n",
    "                                                              test_size=test_proportion, \n",
    "                                                              random_state=seed)\n",
    "\n",
    "train = GeneratedDataset(z_train, Sigmas_train)\n",
    "test = GeneratedDataset(z_test, Sigmas_test)\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=z_test.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc942cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144b4e05f47b4684a5e2f7c25411dc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a24345cdc1a42f1a96ab3db55433495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialise prediction model\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "PR = NNMatrixOutput(n_context = n_context, hidden_size = hidden_size, n_vars=n_vars)\n",
    "\n",
    "PR_no_robust = NNMatrixOutput(n_context = n_context, hidden_size = hidden_size, n_vars=n_vars)\n",
    "\n",
    "\n",
    "# Initialise optimisers\n",
    "optimiser = torch.optim.Adam(PR.parameters(), lr = learning_rate)\n",
    "\n",
    "optimiser_no_robust = torch.optim.Adam(PR_no_robust.parameters(), lr = learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for batch in train_dataloader:\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        pred = PR(batch[\"z\"])\n",
    "        \n",
    "        x_det_opt, = ROB_1(pred[\"c\"],pred[\"Sigma\"]) # here we predict the square root of the matrix\n",
    "        x_xt_opt = x_det_opt @ x_det_opt.permute(0,2,1)\n",
    "        \n",
    "        wc_opt = ROB_2(x_xt_opt,batch[\"Sigma\"])\n",
    "        worst_case_sigma = wc_opt[1]\n",
    "        \n",
    "        loss = 0 \n",
    "        loss_p = (-c.T@x_det_opt).view(-1,1)\n",
    "        loss += loss_p\n",
    "        \n",
    "        loss_violation = 0\n",
    "        \n",
    "        violation = torch.sum((worst_case_sigma@x_xt_opt).diagonal(offset=0, dim1=-2, dim2=-1),axis = -1) - risk_limit\n",
    "        loss_violation = (nn.ReLU()(violation)).view(-1,1)\n",
    "        loss += loss_violation\n",
    "        \n",
    "        loss = torch.sum(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "            \n",
    "# Training loop for no robust case\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        # Optimise a network without robust regulariser\n",
    "        optimiser_no_robust.zero_grad()\n",
    "    \n",
    "        pred_nr = PR_no_robust(batch[\"z\"])\n",
    "        \n",
    "        x_det_opt, = ROB_1(pred_nr[\"c\"],pred_nr[\"Sigma\"]) # here we predict the square root of the matrix\n",
    "        x_xt_opt = x_det_opt @ x_det_opt.permute(0,2,1)\n",
    "        \n",
    "        \n",
    "        loss = 0 \n",
    "        loss_p = (-c.T@x_det_opt).view(-1,1)\n",
    "        loss += loss_p\n",
    "        \n",
    "        loss_violation = 0\n",
    "        \n",
    "        violation = torch.sum((batch[\"Sigma\"]@x_xt_opt).diagonal(offset=0, dim1=-2, dim2=-1),axis = -1) - risk_limit\n",
    "        loss_violation = (nn.ReLU()(violation)).view(-1,1)\n",
    "        loss += loss_violation\n",
    "        \n",
    "        loss = torch.sum(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimiser_no_robust.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b752f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        pred = PR(batch[\"z\"])\n",
    "        pred_nr = PR_no_robust(batch[\"z\"])\n",
    "        \n",
    "        x_ro, = ROB_1(pred[\"c\"],pred[\"Sigma\"])\n",
    "        xxt_ro = x_ro @ x_ro.permute(0,2,1)\n",
    "        violation_ro = torch.sum((batch[\"Sigma\"]@xxt_ro).diagonal(offset=0, dim1=-2, dim2=-1),axis = -1) - risk_limit\n",
    "\n",
    "        x_nr, = ROB_1(pred_nr[\"c\"],pred_nr[\"Sigma\"])\n",
    "        xxt_nr = x_nr @ x_nr.permute(0,2,1)\n",
    "        violation_nr = torch.sum((batch[\"Sigma\"]@xxt_nr).diagonal(offset=0, dim1=-2, dim2=-1),axis = -1) - risk_limit\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff551479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of constraint violations in test set with ROB_2: 0 out of 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of constraint violations in test set with ROB_2: {torch.sum(violation_ro >= 0).numpy()} out of {z_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204ab32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of constraint violations in test set without ROB_2: 13 out of 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of constraint violations in test set without ROB_2: {torch.sum(violation_nr >= 0).numpy()} out of {z_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed8a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average change in the objective is 37.3 %.\n"
     ]
    }
   ],
   "source": [
    "average_increase = torch.mean(((-c.T@x_nr) - (-c.T@x_ro))/(-c.T@x_nr)).numpy()*100\n",
    "print(f\"The average change in the objective is\", \"%.1f\" % average_increase,\"%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
